import os
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
import wfdb
import ast
from scipy.signal import resample, butter, sosfiltfilt, iirnotch
from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer
from sklearn import preprocessing
import re
import h5py
from scipy.ndimage import zoom


# DATA PROCESSING STUFF
def load_dataset(path, sampling_rate, release=False):
    if path.split('/')[-2] == 'ptbxl':
        # load and convert annotation data
        Y = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')
        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))

        # Load raw signal data
        X = load_raw_data_ptbxl(Y, sampling_rate, path)

    elif path.split('/')[-2] == 'CPSC':
        # load and convert annotation data
        Y = pd.read_csv(path + 'cpsc_database.csv', index_col='ecg_id')
        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))

        # Load raw signal data
        X = load_raw_data_cpsc(Y, sampling_rate, path)

    elif path.split('/')[-2] == 'SPH':
        # load and convert annotation data
        Y = pd.read_csv(path + 'sph_database.csv', index_col='ECG_ID')
        Y.AHA_Code = Y.AHA_Code.apply(lambda x: re.split(r'[;]', x))
        # Load raw signal data
        X = load_raw_data_sph(Y, sampling_rate, path)

    return X, Y


def load_raw_data_sph(df, sampling_rate, path):
    if sampling_rate == 100:
        if os.path.exists(path + 'raw100.npy'):
            new_data = np.load(path + 'raw100.npy', allow_pickle=True)
        else:
            data = [h5py.File(path + 'records500/' + str(f) + '.h5', 'r')['ecg'][()] for f in tqdm(df.index)]
            new_signals = []
            for i in range(len(data)):
                data[i] = data[i][:, :5000]
                new_signals.append(data[i])
            ori_data = np.array(new_signals)
            new_data = []
            for i in range(ori_data.shape[0]):
                down_sig = np.array([zoom(channel.astype(np.float32), .2) for channel in ori_data[i]])
                new_data.append(down_sig)
            new_data = np.array(new_data)
            # print(new_data.shape)  # (25770, 12, 1000)
            pickle.dump(new_data, open(path + 'raw100.npy', 'wb'), protocol=4)
        return new_data

    elif sampling_rate == 500:
        if os.path.exists(path + 'raw500.npy'):
            data = np.load(path + 'raw500.npy', allow_pickle=True)
        else:
            data = [h5py.File(path + 'records500/' + str(f) + '.h5',  'r')['ecg'][()] for f in tqdm(df.index)]
            # print(len(data))  # 25770
            new_signals = []
            for i in range(len(data)):
                data[i] = data[i][:, :5000]
                # print(data[i].shape)  # (12, 5000)
                new_signals.append(data[i])
            data = np.array(new_signals)
            # print(data.shape)  # (25770, 12, 5000)
            pickle.dump(data, open(path + 'raw500.npy', 'wb'), protocol=4)
        return data


def load_raw_data_cpsc(df, sampling_rate, path):
    if sampling_rate == 100:
        if os.path.exists(path + 'raw100.npy'):
            data = np.load(path + 'raw100.npy', allow_pickle=True)
        else:
            data = [wfdb.rdsamp(path + 'records100/' + str(f)) for f in tqdm(df.index)]
            data = np.array([signal for signal, meta in data])
            pickle.dump(data, open(path + 'raw100.npy', 'wb'), protocol=4)
    elif sampling_rate == 500:
        if os.path.exists(path + 'raw500.npy'):
            data = np.load(path + 'raw500.npy', allow_pickle=True)
        else:
            data = [wfdb.rdsamp(path + 'records500/' + str(f)) for f in tqdm(df.index)]
            data = np.array([signal for signal, meta in data])
            pickle.dump(data, open(path + 'raw500.npy', 'wb'), protocol=4)
    return data


def load_raw_data_ptbxl(df, sampling_rate, path):
    if sampling_rate == 100:
        if os.path.exists(path + 'raw100.npy'):
            data = np.load(path + 'raw100.npy', allow_pickle=True)
        else:
            data = [wfdb.rdsamp(path + f) for f in tqdm(df.filename_lr)]
            data = np.array([signal for signal, meta in data])
            pickle.dump(data, open(path + 'raw100.npy', 'wb'), protocol=4)
    elif sampling_rate == 500:
        if os.path.exists(path + 'raw500.npy'):
            data = np.load(path + 'raw500.npy', allow_pickle=True)
        else:
            data = [wfdb.rdsamp(path + f) for f in tqdm(df.filename_hr)]
            data = np.array([signal for signal, meta in data])
            pickle.dump(data, open(path + 'raw500.npy', 'wb'), protocol=4)
    return data


def sph_compute_label_aggregations(df, folder, ctype):
    df['aha_codes_len'] = df.AHA_Code.apply(lambda x: len(x))

    aggregation_df = pd.read_csv(folder + 'code.csv', index_col=0)

    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:

        # 44
        def aggregate_all_diagnostic(y_dic):
            tmp = []
            for key in y_dic:
                if "+" in key:
                    key = key.split('+')[0]
                tmp.append(key)
            return list(set(tmp))

        # 11
        def aggregate_subdiagnostic(y_dic):
            tmp = []
            for key in y_dic:
                if "+" in key:
                    key = key.split('+')[0]
                match_item = diag_agg_df[diag_agg_df['Code'] == int(key)]
                category_item = match_item.index.item()
                tmp.append(category_item)
            return list(set(tmp))

        # 2
        def aggregate_diagnostic(y_dic):
            tmp = []
            for key in y_dic:
                if "+" in key:
                    key = key.split('+')[0]
                if key == '1':
                    category_item = 'Normal'
                else:
                    category_item = 'AbNormal'
                tmp.append(category_item)
            return list(set(tmp))

        diag_agg_df = aggregation_df
        if ctype == 'diagnostic':
            df['diagnostic'] = df.AHA_Code.apply(aggregate_all_diagnostic)
            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))
        elif ctype == 'subdiagnostic':
            df['subdiagnostic'] = df.AHA_Code.apply(aggregate_subdiagnostic)
            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))
        elif ctype == 'superdiagnostic':
            df['superdiagnostic'] = df.AHA_Code.apply(aggregate_diagnostic)
            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))

    return df


def compute_label_aggregations(df, folder, ctype):
    df['scp_codes_len'] = df.scp_codes.apply(lambda x: len(x))

    aggregation_df = pd.read_csv(folder + 'scp_statements.csv', index_col=0)

    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:

        def aggregate_all_diagnostic(y_dic):
            tmp = []
            for key in y_dic.keys():
                if key in diag_agg_df.index:
                    tmp.append(key)
            return list(set(tmp))

        def aggregate_subdiagnostic(y_dic):
            tmp = []
            for key in y_dic.keys():
                if key in diag_agg_df.index:
                    c = diag_agg_df.loc[key].diagnostic_subclass
                    if str(c) != 'nan':
                        tmp.append(c)
            return list(set(tmp))

        def aggregate_diagnostic(y_dic):
            tmp = []
            for key in y_dic.keys():
                if key in diag_agg_df.index:
                    c = diag_agg_df.loc[key].diagnostic_class
                    if str(c) != 'nan':
                        tmp.append(c)
            return list(set(tmp))

        diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]
        if ctype == 'diagnostic':
            df['diagnostic'] = df.scp_codes.apply(aggregate_all_diagnostic)
            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))
        elif ctype == 'subdiagnostic':
            df['subdiagnostic'] = df.scp_codes.apply(aggregate_subdiagnostic)
            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))
        elif ctype == 'superdiagnostic':
            df['superdiagnostic'] = df.scp_codes.apply(aggregate_diagnostic)
            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))
    elif ctype == 'form':
        form_agg_df = aggregation_df[aggregation_df.form == 1.0]

        def aggregate_form(y_dic):
            tmp = []
            for key in y_dic.keys():
                if key in form_agg_df.index:
                    c = key
                    if str(c) != 'nan':
                        tmp.append(c)
            return list(set(tmp))

        df['form'] = df.scp_codes.apply(aggregate_form)
        df['form_len'] = df.form.apply(lambda x: len(x))
    elif ctype == 'rhythm':
        rhythm_agg_df = aggregation_df[aggregation_df.rhythm == 1.0]

        def aggregate_rhythm(y_dic):
            tmp = []
            for key in y_dic.keys():
                if key in rhythm_agg_df.index:
                    c = key
                    if str(c) != 'nan':
                        tmp.append(c)
            return list(set(tmp))

        df['rhythm'] = df.scp_codes.apply(aggregate_rhythm)
        df['rhythm_len'] = df.rhythm.apply(lambda x: len(x))
    elif ctype == 'all':
        df['all_scp'] = df.scp_codes.apply(lambda x: list(set(x.keys())))

    return df


def select_data(XX, YY, ctype, min_samples):
    # convert multilabel to multi-hot
    mlb = MultiLabelBinarizer()

    if ctype == 'diagnostic':
        X = XX[YY.diagnostic_len > 0]
        Y = YY[YY.diagnostic_len > 0]
        mlb.fit(Y.diagnostic.values)
        y = mlb.transform(Y.diagnostic.values)
    elif ctype == 'subdiagnostic':
        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))
        X = XX[YY.subdiagnostic_len > 0]
        Y = YY[YY.subdiagnostic_len > 0]
        mlb.fit(Y.subdiagnostic.values)
        y = mlb.transform(Y.subdiagnostic.values)
    elif ctype == 'superdiagnostic':
        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))
        X = XX[YY.superdiagnostic_len > 0]
        Y = YY[YY.superdiagnostic_len > 0]
        mlb.fit(Y.superdiagnostic.values)
        y = mlb.transform(Y.superdiagnostic.values)
    else:
        pass

    return X, Y, y, mlb


def sph_select_data(XX, YY, ctype, min_samples):
    # convert multilabel to multi-hot
    mlb = MultiLabelBinarizer()

    if ctype == 'diagnostic':
        counts = pd.Series(np.concatenate(YY.diagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.diagnostic = YY.diagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['diagnostic_len'] = YY.diagnostic.apply(lambda x: len(x))
        X = XX[YY.diagnostic_len > 0]
        Y = YY[YY.diagnostic_len > 0]
        mlb.fit(Y.diagnostic.values)
        y = mlb.transform(Y.diagnostic.values)
    elif ctype == 'subdiagnostic':
        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))
        X = XX[YY.subdiagnostic_len > 0]
        Y = YY[YY.subdiagnostic_len > 0]
        mlb.fit(Y.subdiagnostic.values)
        y = mlb.transform(Y.subdiagnostic.values)
    elif ctype == 'superdiagnostic':
        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))
        X = XX[YY.superdiagnostic_len > 0]
        Y = YY[YY.superdiagnostic_len > 0]
        mlb.fit(Y.superdiagnostic.values)
        y = mlb.transform(Y.superdiagnostic.values)
    else:
        pass

    return X, Y, y, mlb

def select_data_label(XX, YY, ctype_p, ctype_s, min_samples, folder):
    # convert multilabel to multi-hot
    mlb_p = MultiLabelBinarizer()
    mlb_s = MultiLabelBinarizer()

    if ctype_p == 'superdiagnostic' and ctype_s == 'subdiagnostic':
        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))
        X = XX[YY.superdiagnostic_len > 0]
        Y_p = YY[YY.superdiagnostic_len > 0]
        mlb_p.fit(Y_p.superdiagnostic.values)
        y_p = mlb_p.transform(Y_p.superdiagnostic.values)

        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))
        Y_s = YY[YY.subdiagnostic_len > 0]
        mlb_s.fit(Y_s.subdiagnostic.values)
        y_s = mlb_s.transform(Y_s.subdiagnostic.values)

        # print(y_p.shape)  # (21430, 5)
        # print(y_s.shape)  # (21430, 23)
        # print(mlb_p.classes_)  # ['CD' 'HYP' 'MI' 'NORM' 'STTC']
        # print(mlb_s.classes_)  # ['AMI' 'CLBBB' 'CRBBB' 'ILBBB' 'IMI' 'IRBBB' 'ISCA' 'ISCI' 'ISC_' 'IVCD' 'LAFB/LPFB' 'LAO/LAE' 'LMI' 'LVH' 'NORM' 'NST_' 'PMI' 'RAO/RAE' 'RVH' 'SEHYP' 'STTC' 'WPW' '_AVB']

    elif ctype_p == 'subdiagnostic' and ctype_s == 'diagnostic':
        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()
        counts = counts[counts > min_samples]
        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))
        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))
        X = XX[YY.subdiagnostic_len > 0]
        Y_p = YY[YY.subdiagnostic_len > 0]
        mlb_p.fit(Y_p.subdiagnostic.values)
        y_p = mlb_p.transform(Y_p.subdiagnostic.values)

        Y_s = YY[YY.diagnostic_len > 0]
        mlb_s.fit(Y_s.diagnostic.values)
        y_s = mlb_s.transform(Y_s.diagnostic.values)

        # print(y_p.shape)  # (21430, 23)
        # print(y_s.shape)  # (21430, 44)
        # print(mlb_p.classes_)  # ['AMI' 'CLBBB' 'CRBBB' 'ILBBB' 'IMI' 'IRBBB' 'ISCA' 'ISCI' 'ISC_' 'IVCD' 'LAFB/LPFB' 'LAO/LAE' 'LMI' 'LVH' 'NORM' 'NST_' 'PMI' 'RAO/RAE' 'RVH' 'SEHYP' 'STTC' 'WPW' '_AVB']
        # print(mlb_s.classes_)  # ['1AVB' '2AVB' '3AVB' 'ALMI' 'AMI' 'ANEUR' 'ASMI' 'CLBBB' 'CRBBB' 'DIG' 'EL' 'ILBBB' 'ILMI' 'IMI' 'INJAL' 'INJAS' 'INJIL' 'INJIN' 'INJLA' 'IPLMI' 'IPMI' 'IRBBB' 'ISCAL' 'ISCAN' 'ISCAS' 'ISCIL' 'ISCIN' 'ISCLA' 'ISC_' 'IVCD' 'LAFB' 'LAO/LAE' 'LMI' 'LNGQT' 'LPFB' 'LVH' 'NDT' 'NORM' 'NST_' 'PMI' 'RAO/RAE' 'RVH' 'SEHYP' 'WPW']

    else:
        pass

    return X, Y_p, y_p, mlb_p, y_s, mlb_s


def preprocess_signals(X_train, X_validation, X_test):
    # Standardize data such that mean 0 and variance 1
    ss = StandardScaler()
    ss.fit(np.vstack(X_train).flatten()[:, np.newaxis].astype(float))

    return apply_standardizer(X_train, ss), apply_standardizer(X_validation, ss), apply_standardizer(X_test, ss)


def apply_standardizer(X, ss):
    X_tmp = []
    for x in X:
        x_shape = x.shape
        X_tmp.append(ss.transform(x.flatten()[:, np.newaxis]).reshape(x_shape))
    X_tmp = np.array(X_tmp)
    return X_tmp


def data_slice(data):
    data_process = []
    for dat in data:
        if dat.shape[0] < 1000:
            # dat = np.pad(dat, (0, 1000 - dat.shape[0]), 'constant', constant_values=0)
            dat = resample(dat, 1000, axis=0)
        elif dat.shape[0] > 1000:
            dat = dat[:1000, :]
            # dat = resample(dat, 1000, axis=0)
        if dat.shape[1] != 12:
            dat = dat[:, 0:12]

        data_process.append(dat)
    return np.array(data_process)

def bandpass_notch_filter(data, fs=100):
    """
    Apply 0.5–45 Hz Butterworth band-pass filter and 50 Hz notch filter to ECG signals.
    
    Parameters:
        data (ndarray): Input ECG data, shape [time, leads].
        fs (int): Sampling frequency (default 100 Hz).
    
    Returns:
        ndarray: Filtered ECG data.
    """
    # 1. Band-pass filter (0.5–45 Hz, 2nd-order Butterworth)
    sos = butter(2, [0.5, 45], btype='bandpass', fs=fs, output='sos')
    filtered = sosfiltfilt(sos, data, axis=0)

    return filtered



